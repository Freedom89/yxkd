---
title: CS6741 OMSCS - Machine Learning
date: 2024-05-02 0000:00:00 +0800
categories: [Courses, Gatech]
tags: [courses, omscs]     # TAG names should always be lowercase
math: true
toc: true
mermaid: true
---


## Overall Review

> If I had taken this as my first class (instead of 7/8 - sunk cost fallacy), I would have dropped out of OMSCS.
> I apologize in advance if this review seems negative, and it is not meant to bash or criticize the course in anyway.
>
> Again, It's always easier to be a critic than a creator!
{: .prompt-danger }

With that out of the way, I took this course along with [CS 8803 O21: GPU Hardware and Software](https://omscs.gatech.edu/cs-8803-o21-gpu-hardware-and-software) as my 7th and 8th module
of my 5th semester. This module is **not about the algorithms or machine leraning**, but the **behavior** of the machine learning algorithms based on the **data**. It is more of a "data science/analytics" course, where you are using existing tools (`sklearn`, `pytorch`), and trying to see the effects of each knob of each algorithm and how it interacts with the data.
If you are taking this course and wish to do well, do check out [other's reviews](#others-reviews) as well as the [assignments](#assignments) section. 

> For a little more context, The assignments is structured in an open ended way, and the rubrics is not disclosed but they do provide FAQs on what you **need** to include and
> those serves as a purpose. I will elaborate more on this in the [assignments](#assignments) section.
> 
> Initially for A1, I "made a mistake (at least for the purpose of this course)" and focused too much on the algorithms, "stating" the findings on a superficial level and got 73. 
> After I figured out what they really wanted, I got 100, 103 (98+3) and 95 for the remaining assignments. 
{: .prompt-tip }

To understand what I meant by a "data science and analytics" course, do watch this [youtube video featuring Professor Charles Isbell,Professor Michael Littmann and Lex Fridman](https://www.youtube.com/watch?v=yzMVEbs8Zz0&t=606s), this link starts at a specific time, watch until 14.50. To be fair, I do witness in industry,
where "(some) software engineers (or management)" think in a binary/simplistic way and unable able to be "flexible"; if this is the input, this should be the expected output - as though things are deterministic, but this is not the case for machine learning systems which has a strong dependency on your data. I have also been in conversations where engineers are discussing which algorithms to use, without even understanding how the data is collected or if the data is even being collected :shrug:. 

For this reason, coupled with the hidden rubrics, this course does give you quite a little bit (read: **`ALOT`**) of anxiety which I personally feel is unnecessary; how do you know whether you have done enough and "answer enough questions" and whether your answers are "interesting" enough? Which again, can be subjective.

> Nonetheless, the course staff is **trying very hard** to make things better, for example in spring 2024 vs fall 2024, the professor removed midterms to help in student's work life balance.
>
> Starting in spring 2024, the TAs team also has been putting out [blog articles](https://sites.gatech.edu/omscs7641/) to explain certain concepts.
>
> Compared to times even prior to that (Prof Charles Isbell) the only way to get information was to attend office hours, now, TAs provides a "FAQ" section as a initial starting point and what you should include in your report.
>
> They also allow chatGPT / generative AI in this class, which really helped!
{: .prompt-info }

## Other(s) Reviews

* [A fellow student's review on this course by Nikhilkapila](https://nikhilkapila.substack.com/p/avoid-overfitting-a-practical-guide)
* [Another student's review and survival guide by Anika Neela](https://anikaneela.medium.com/cs-7641-survival-guide-strategies-and-resources-for-omscs-machine-learning-61ad2b335a20)
* [A reddit post: Why CS7641 is an awesome class and some tips to succeed.](https://www.reddit.com/r/OMSCS/comments/18oc5ad/why_cs7641_is_an_awesome_class_and_some_tips_to/)

## Grades Distribution

|Type(Weightage)|Mean|Lower Quartile|Median|Upper Quartile|High|
|:--|:--|:--|:--|:--|:--|:--|
|Assignment 1 (15%)|70.74|58|74|85|105|
|Assignment 2 (15%)|67.51|53|72|86|105|
|Assignment 3 (20%)|62.54|51|64|78|105|
|Assignment 4 (20%)|67.64|56|72|84|105|
|Final Exam (25%)|37.59|35.22|39.35|43.65|53.3|
|Reading <br> Writing Quiz (5%)|20.13|21.5|22|22|22|
|Combined Grade|69.01|61.92|71.93|79.91|97.53|

The instructors do not release the cutoffs for A/B, but based on students contribution in an attempt to reverse engineer the cutoffs, it seems to be 70+ For an A, and in the mid 50s will get you a B for Spring 2024.

There are also two extra credit opportunities, one for ed participation and one for attempting a problem set, intended to help you for the final exam.

## Lectures

These are the lectures to watch for each week, they shouldn't take you more than 3 hours each week:

* `SL` : Supervised learning
* `UL` : Unsupervised learning (durh)
* `RL` : Reinforcement Learning

|||
|:--|:--|
|Week 1|SL 1 - Decision Trees|
|Week 2|SL 2 - Regression & Classification <br> SL 3 - Neural Networks|
|Week 3|SL 4 - Instance Based Learning <br> SL 5 - Ensemble Learning|
|Week 4|SL 6 - Kernel Methods and SVMs|
|Week 5|SL 7 - Computational Learning Theory <br> SL 8 - VC Dimensions|
|Week 6|SL 9 - Bayesian Learning <br> SL 10 - Bayesian Inference|
|Week 7|UL 1 - Randomized Optimization <br> UL 5 - Information Theory|
|Week 9|UL 2 - Clustering|
|Week 10|UL 3 - Feature Selection <br> UL 4 - Feature Transformation|
|Week 11|RL 1 - Markov Decision Processes|
|Week 12|RL 2 - Reinforcement Learning|
|Week 14|RL 3, RL 4 - Game Theory|

There are also some assigned readings **which I read none of them** (GPU readings were significantly more interesting *ahem*). 

In my opinion the lectures touch only the surface, and certain sections can be quite painful to watch. I suggest watching them at 2x speed and
then finding your own resources to understand what is going on.

For example, I watched [David Silver Lectures 1-4](https://www.davidsilver.uk/teaching/) to understand reinforcement learning. I also did not watch certain part of the lectures, and opt to 
just read from study guide(s) instead.

## Useful Python Concepts

Do get additional computation resources, they will help you in your tuning and experiments. I am fortunate enough to have a [powerful pc with 32 threads](../deep-learning-rig)!

On top of that, spend time to understand the following concepts:

* Using [joblib](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html) to distribute your code, this is especially important / useful for A2 and A4.
* Using [Pickle](https://python.readthedocs.io/en/latest/library/pickle.html) to save/reload your results from your experiments - For me, I have a script to generate the results which ran overnight, and another script for analysis. 

## Quiz

There are some assigned readings that are meant for students to "learn" how to write better (at least in an academic setting). I... only attempted the quiz after assignment 4. 
There are unlimited attempts and the best score is taken.

## Assignments

Do check out [other's reviews](#others-reviews) on how to do well, but the top 2 suggestions I have are following the [FAQs](#faqs) and choosing an [efficient report template](#report-templates). I will also try to provide my thought process on what made me do well for assignments 2-4.

Generally, on a very high level:

* Always ask yourself before you start any coding "what do you expect to see? and why?"
* If its possible, come up with a hypothesis and state it explicitly, ideally there should be some EDA involved to justify why your hypothesis is fair.
* Link back to the data/structure of the problem if its possible (more on this later), does the nature of the problem affects anything? What about the data?
* Always link back to lecture material or strongly grounded theory (Such as PCA is dependent on the variance of the data, so is bias towards features that have higher variance)
* **Compare whenever/wherever you can and analyze them**; for instance, model A perform than model B for this data, why? This data is more suitable for GMM instead of Kmeans, why? 

### FAQs

A fellow student remarked in ed discussions that (if you want to do well) you should follow the FAQs like a doctrine. If they ask or "casually" say you "should include" learning and validation curves, you **are highly recommended** to do so, else incur a huge penalty. 

### Report Templates 

Use the IEE latex for your assignments! Templates are already provided by [KYLE NAKAMURA in his github](https://github.com/knakamura13/cs7641-ml-study-materials-2023), they really helped in saving space.

![image](../../../assets/posts/gatech/ml/report.png){: width='400' height='400'}
*IEEE template, double columns and being able to squeeze 2 images side by side!*

### Datasets

* [awesome public datasets](https://github.com/awesomedata/awesome-public-datasets)
* [Kaggle](https://www.kaggle.com/datasets)
* [UC Irvine Machine Learning datasets](https://archive.ics.uci.edu/)


### Assignment 1

Supervised Learning
* Relevant Lectures: SL1-SL6
* Page limit: 10

> **DO NOT** use `pytorch` and use `sklearn` if you can. (Yes, I know by using `sklearn` it is hard to monitor log loss by epoch)
>
> You will understand why the moment you hit Assignment 2 :smirk:
{: .prompt-danger }

![image](../../../assets/posts/gatech/ml/project1.png){: width='400' height='400'}


### Assignment 2

Randomized algorithm
* Relevant Lectures: UL1
* Page limit: 10

![image](../../../assets/posts/gatech/ml/project2.png){: width='400' height='400'}

```python
    problem = mlrose.DiscreteOpt(
        length=size, fitness_fn=mlrose.FourPeaks(t_pct=t_pct), maximize=True
    )
    problem.set_mimic_fast_mode(fast_mode=True)
```

https://github.com/hiive/mlrose.git


### Assignment 3

Unsupervised learning and dimensionality Reduction
* Relevant Lectures: UL2
* Page limit: 8

![image](../../../assets/posts/gatech/ml/project3.png){: width='400' height='400'}



### Assignment 4

Reinforcement Learning
* Relevant Lectures: RL1-RL2
* Page limit: 8


https://github.com/jlm429/bettermdptools/

![image](../../../assets/posts/gatech/ml/project4.png){: width='400' height='400'}

![image](../../../assets/posts/gatech/ml/project4_2.png){: width='400' height='400'}

## Final exam

The final exam consist of 57 questions, with 19 questions each from SL,UL and RL. Personally I used notes from:

* [teapowered ML notes](https://teapowered.dev/assets/ml-notes.pdf)
* [monzersaleh ML notes](https://monzersaleh.github.io/GeorgiaTech/CS7641_MachineLearning.html)

It turns out that my hypothesis was correct - As the assignments covered SL1-6, UL1-2, and RL1-2, the finals mainly focused from SL7-10, UL3-4, RL3-4.
There were a higher number of questions on bayesian, information theory, feature selection / Transformation and game theory. 

Looks like you have no escape!








