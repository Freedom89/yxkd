---
toc: true
mermaid: true
hidden: true
math: true
---

### Readings

* [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)
* [BERT Paper](https://arxiv.org/pdf/1810.04805.pdf)
* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

### Softmax and Preview of Attention

### Attention

### Transformers

### Guest Lecturers




<!-- {% include embed/youtube.html id='10oQMHadGos' %} -->
