---
toc: true
mermaid: true
hidden: true
math: true
---

### Introduction to Transformers

### Transformers: Combining 3 Concepts

### Transformer Encoder

### Transformers: Self-Attention

### Self-Attention Process: Outputs

### Multi-Headed Self-Attention

### Transformer Decoder

### BERT

### BERT Training Examples

### GPT

### GPT Examples

### Fine-Tuning and Reinforcement Learning

### Instruction Tuning

### Reinforcement Learning

### Reinforcement Learning for Text

### Code Analysis Introduction

### Code Analysis: Self-Attention Module

### Code Analysis: Decoder

[Transformer Walkthrough Code](https://github.com/markriedl/transformer-walkthrough)


<!-- {% include embed/youtube.html id='10oQMHadGos' %} -->